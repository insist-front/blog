---
title: Spark
date: 2021-11-1
keys:
 - 'B96B9F951DCE66A273F7DA5408DF7B1F'
tags:
 - Spark
categories:
 -  大数据
---



# Spark简介

Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎

# Spark核心模块

![](./images/1.png)

1. **Spark Core**

   Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的

2. **Spark SQL**

   Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。

3. **Spark Streaming**

   Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API

4. **Spark MLlib**

   MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。

5. **Spark GraphX**

   GraphX 是 Spark 面向图计算提供的框架与算法库

# Spark快速上手

1. Scala环境

2. 添加Spark依赖

   ```xml
   <dependencies>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-core_2.12</artifactId>
    	<version>3.0.0</version>
    </dependency>
   </dependencies>
   ```

3. 编写代码

   ```scala
   object Spark_02_Spark_WordCount {
   
     def main(args: Array[String]): Unit = {
       // 创建Spark运行配置对象
       val sparkConf = new SparkConf().setMaster("local").setAppName("wordcount")
       //创建Spark上下文对象
       val sparkContext = new SparkContext(sparkConf)
   
       // 业务逻辑
       //1.读取文件数据
       val wordRDD:RDD[String] = sparkContext.textFile("bigdata_08_spark/datas")
       // 扁平化
       val wordFlatMapRDD: RDD[String] = wordRDD.flatMap(_.split(" "))
       // 每个元素打上 1
       val value: RDD[(String, Int)] = wordFlatMapRDD.map((_, 1))
       // Spark提供根据key 将value做reduce操作
       val res: RDD[(String, Int)] = value.reduceByKey(_ + _)
       // 打印
       res.foreach(println)
       //关闭资源
       sparkContext.stop()
     }
   }
   ```

   

# 去除日志信息

执行过程中，会产生大量的执行日志，如果为了能够更好的查看程序的执行结果，可以在项

目的 resources 目录中创建 log4j.properties 文件，并添加日志配置信息：

```properties
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd 
HH:mm:ss} %p %c{1}: %m%n
# Set the default spark-shell log level to ERROR. When running the spark-shell, 
the
# log level for this class is used to overwrite the root logger's log level, so 
that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=ERROR
# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent 
UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
```

# Spark运行环境

Spark 作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行, 在国内工作中主流的环境为 Yarn，不过逐渐容器式环境也慢慢流行起来。接下来，我们就分别看看不同环境下 Spark 的运行

![](./images/2.png)

## Local模式

所谓的 Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等，之前在 IDEA 中运行代码的环境我们称之为开发环境，不太一样

1. 解压文件

2. 解压文件

   ```shell
   tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
   cd /opt/module 
   mv spark-3.0.0-bin-hadoop3.2 spark-local
   ```

3. 启动Local环境

   ```shell
   bin/spark-shell
   ```

   运行后会在问价中创建data文件夹

4. 执行代码

   ![](./images/3.png)

5. 通过浏览器访问

   ![](./images/4.png)